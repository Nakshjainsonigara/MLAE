{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Newton's Method for Linear Regression",
   "id": "888f42ff8aef61c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following document presents the implementation of Newton's method for minimizing the cost function of linear regression. Newton's method works by finding the point where the derivative of the function equals zero. Starting with an initial guess for theta, it calculates the derivative at that point to locate critical points. Unlike gradient descent, this method also computes the second derivative (double derivative) to determine the slope's behavior and decide whether to move in the direction of minimization or maximization.\n",
    "\n",
    "In vector form, the update rule is given by theta minus the dot product of the inverse of the Hessian matrix (the second derivative of the cost function) and the gradient (the partial derivative of the function). This technique is typically applied in logistic regression to maximize the log-likelihood. Here, I have attempted to implement it for minimizing the cost function of linear regression."
   ],
   "id": "56de5ce1eb949535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T18:21:48.962379Z",
     "start_time": "2024-12-11T18:21:48.954856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import add_dummy_feature"
   ],
   "id": "3e7c007a394771c0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T18:23:03.243512Z",
     "start_time": "2024-12-11T18:23:03.201650Z"
    }
   },
   "source": [
    "# I have used housing data \n",
    "def load_data():\n",
    "    tarball_path = Path(\"data/housing.csv\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"data\").mkdir(parents=True,exist_ok=True)\n",
    "        url='https://github.com/ageron/data/raw/main/housing.tgz'\n",
    "        r=requests.get(url)\n",
    "        with open(tarball_path,\"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(\"data\")\n",
    "    return pd.read_csv(\"data/housing/housing.csv\")\n",
    "df = load_data()\n",
    "df = df.dropna()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T18:32:05.490053Z",
     "start_time": "2024-12-11T18:32:05.472501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X=df[[\"median_income\"]]\n",
    "y=df[[\"median_house_value\"]]\n",
    "X_b = add_dummy_feature(X)\n",
    "\n",
    "m,n = X_b.shape\n",
    "theta = np.zeros((n,1))\n",
    "\n",
    "def cost_fuctions(X,y,theta):\n",
    "    h0x = np.dot(X,theta)\n",
    "    j0 = (1/(2*len(y)))*np.sum((h0x-y)**2)\n",
    "    return j0\n",
    "    \n",
    "def gradient(X,y,theta):\n",
    "    m=len(y)\n",
    "    grad = (1/m)*(X.T.dot(X.dot(theta)-y))\n",
    "    return grad\n",
    "\n",
    "def hessian(X):\n",
    "    m = X.shape[0]\n",
    "    return (1 / m) * X.T.dot(X)\n",
    "\n",
    "\n",
    "def newtons_method(X,y,theta,tol=1e-6,max_iter=1000):\n",
    "    for i in range(max_iter):\n",
    "        grad = gradient(X,y,theta)\n",
    "        hess = hessian(X)\n",
    "        theta_new = theta - np.linalg.inv(hess).dot(grad)\n",
    "        \n",
    "        if np.linalg.norm(theta_new - theta) < tol:\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        theta = theta_new\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta_optimized = newtons_method(X_b,y,theta)\n",
    "\n",
    "print(\"Optimized parameters:\")\n",
    "print(\"Intercept (theta_0):\", theta_optimized[0][0])\n",
    "print(\"Slope (theta_1):\", theta_optimized[1][0])"
   ],
   "id": "ccd48a874066305e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations.\n",
      "Optimized parameters:\n",
      "Intercept (theta_0): 44906.36945088747\n",
      "Slope (theta_1): 41837.066075621886\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T18:33:35.786270Z",
     "start_time": "2024-12-11T18:33:35.776389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "print(\"Intercept of the model:\", model.intercept_)\n",
    "print(\"Coefficients of the model:\", model.coef_)"
   ],
   "id": "f5e98860771fe6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept of the model: [44906.36945088]\n",
      "Coefficients of the model: [[41837.06607562]]\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
